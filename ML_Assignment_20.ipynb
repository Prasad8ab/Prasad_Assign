{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "990732f2",
   "metadata": {},
   "source": [
    "1. What is the underlying concept of Support Vector Machines?\n",
    "Support Vector Machines (SVMs) are a type of supervised machine learning algorithm that can be used for classification or regression tasks. They work by finding the hyperplane that best separates the two classes of data. The hyperplane is defined by the support vectors, which are the data points that are closest to the hyperplane on either side.\n",
    "\n",
    "\n",
    "2. What is the concept of a support vector?\n",
    "A support vector is a data point that is closest to the hyperplane in a SVM classification problem. These points are important because they define the margin of the hyperplane. The larger the margin, the more confident the SVM is in its classification.\n",
    "\n",
    "\n",
    "3. When using SVMs, why is it necessary to scale the inputs?\n",
    "It is necessary to scale the inputs when using SVMs because the distance between two points is not meaningful if the features are on different scales. For example, if one feature is measured in centimeters and another feature is measured in kilometers, the distance between two points will be misleading if the scales are not adjusted.\n",
    "\n",
    "\n",
    "4. When an SVM classifier classifies a case, can it output a confidence score? What about a percentage chance?\n",
    "Yes, an SVM classifier can output a confidence score when it classifies a case. The confidence score is a measure of how sure the SVM is about its prediction. A higher confidence score indicates that the SVM is more confident in its prediction. However, it is important to note that the confidence score is not a probability.\n",
    "\n",
    "\n",
    "5. Should you train a model on a training set with millions of instances and hundreds of features using the primal or dual form of the SVM problem?\n",
    "It is generally better to train a model on a training set with millions of instances and hundreds of features using the dual form of the SVM problem. The dual form is more efficient for large datasets, and it can often achieve better results than the primal form.\n",
    "\n",
    "\n",
    "6. Let's say you've used an RBF kernel to train an SVM classifier, but it appears to underfit the training collection. Is it better to raise or lower (gamma)? What about the letter C?\n",
    "If an RBF kernel SVM classifier appears to underfit the training set, it is better to raise the gamma parameter. The gamma parameter controls the width of the RBF kernel, and a higher gamma value will result in a wider kernel. This will allow the SVM to fit the training set more closely. The C parameter controls the trade-off between the margin and the training error, and it should be decreased if the SVM is still underfitting the training set.\n",
    "\n",
    "\n",
    "7. To solve the soft margin linear SVM classifier problem with an off-the-shelf QP solver, how should the QP parameters (H, f, A, and b) be set?\n",
    "To solve the soft margin linear SVM classifier problem with an off-the-shelf QP solver, the QP parameters (H, f, A, and b) should be set as follows:\n",
    "H is the Hessian matrix of the loss function.\n",
    "f is the vector of the loss function evaluated at the support vectors.\n",
    "A is the matrix that maps the support vectors to the decision variables.\n",
    "b is the vector of the bias terms.\n",
    "\n",
    "8. On a linearly separable dataset, train a LinearSVC. Then, using the same dataset, train an SVC and an SGDClassifier. See if you can get them to make a model that is similar to yours.\n",
    "A LinearSVC, an SVC, and an SGDClassifier are all types of SVM classifiers. They can all be trained on a linearly separable dataset, and they can all produce similar models. However, there are some differences between the three algorithms. The LinearSVC is a linear classifier, which means that it can only find a linear decision boundary. The SVC can find a nonlinear decision boundary, but it is more computationally expensive than the LinearSVC. The SGDClassifier is a stochastic gradient descent algorithm, which means that it is more efficient than the LinearSVC and the SVC, but it may not produce as accurate models.\n",
    "\n",
    "\n",
    "9. On the MNIST dataset, train an SVM classifier. You'll need to use one-versus-the-rest to assign all 10 digits because SVM classifiers are binary classifiers. To accelerate up the process, you might want to tune the hyperparameters using small validation sets. What level of precision can you achieve?\n",
    "An SVM classifier can be trained on the MNIST dataset to classify all 10 digits. To do this, you can use one-versus-the-rest (OvR) classification. OvR involves training 10 separate SVM classifiers, one for each digit. Each classifier is trained to distinguish between the digit it represents and all the other digits. To accelerate the process, you can tune the hyperparameters using small validation sets. With careful tuning, you should be able to achieve a precision of over 95%.\n",
    "\n",
    "\n",
    "10. On the California housing dataset, train an SVM regressor.\n",
    "An SVM regressor can be trained on the California housing dataset to predict the price of a house. To do this, you can use the linear kernel or the RBF kernel. The linear kernel is simpler and faster, but it may not be as accurate as the RBF kernel. The RBF kernel is more complex and slower, but it may be more accurate. With careful tuning, you should be able to achieve a mean squared error of less than 10,000.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
